import logging
import multiprocessing
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from io import StringIO

import requests
from bs4 import BeautifulSoup
from tqdm import tqdm

from ccf_info_by_section import VENUE_TYPES
from tools.logger_utils import setup_logger
from tools.timer import Timer


class PaperQuery:
    def __init__(self, venues: list):
        self.venues = venues
        
        self.timer = Timer()

    def get_soup(self, url: str, local_logger=None):
        try:
            res = requests.get(url, timeout=1000)
            if res.status_code != 200:
                local_logger.warning(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {res.status_code}ï¼ŒURL: {url}")
                return None
        except requests.RequestException as e:
            local_logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}ï¼ŒURL: {url}")
            return None
        return BeautifulSoup(res.text, 'lxml')

    def get_volume_links_journal(self, base_url: str, years: list, local_logger=None):
        soup = self.get_soup(base_url, local_logger)
        if not soup:
            local_logger.warning(f"âš ï¸ æ— æ³•è·å–é¡µé¢å†…å®¹ï¼ŒURL: {base_url}")
            return []

        links_by_year = {year: [] for year in years}
        info_section = soup.find(id="info-section")
        all_volumes = info_section.find_next_sibling("ul")

        for year in years:
            year_tags = [li for li in all_volumes.find_all("li") if year in li.get_text(strip=True)]
            if not year_tags:
                local_logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å¹´ä»½ {year} çš„å·ä¿¡æ¯")
                continue

            for year_tag in year_tags:
                for link in year_tag.find_all("a", href=True):
                    if link['href'].endswith('.html'):
                        links_by_year[year].append(link['href'])

        return links_by_year

    def get_volume_links_conference(self, base_url: str, years: list, local_logger=None):
        soup = self.get_soup(base_url, local_logger)
        if not soup:
            local_logger.warning(f"âš ï¸ æ— æ³•è·å–é¡µé¢å†…å®¹ï¼ŒURL: {base_url}")
            return []

        links_by_year = {year: [] for year in years}
        div_main = soup.find(id="main")
        if not div_main:
            local_logger.warning(f"âš ï¸ æ— æ³•æ‰¾åˆ°ä¸»å†…å®¹åŒºåŸŸï¼ŒURL: {base_url}")
            return []

        h2_total = div_main.find_all("h2")
        for year in years:
            for h2 in h2_total:
                id = h2.get('id')
                if id and year in id:
                    sibling = h2.find_parent().find_next_sibling("ul")
                    volume_links = sibling.find_all("a", href=True, class_="toc-link") if sibling else []
                    for link in volume_links:
                        if link['href'].endswith('.html'):
                            links_by_year[year].append(link['href'])

        return links_by_year

    def fetch_titles(self, volume_url: str, local_logger=None):
        soup = self.get_soup(volume_url, local_logger)
        if not soup:
            local_logger.warning(f"âš ï¸ æ— æ³•è·å–å…·ä½“æ–‡ç« åˆ—è¡¨é¡µå†…å®¹ï¼ŒURL: {volume_url}")
            return []

        articles = []
        for container in soup.find_all("ul", class_="publ-list"):
            articles.extend(container.find_all("span", class_="title"))

        if not articles:
            local_logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ–‡ç« æ ‡é¢˜ï¼ŒURL: {volume_url}")
            return []

        titles = []
        for article in articles:
            text = article.get_text(strip=True)
            if text.endswith(('.', '?')):
                text = text[:-1]
            titles.append(text)
        return titles

    def get_recommended_thread_count(self, task_type="io"):
        cpu_count = multiprocessing.cpu_count()
        return cpu_count * 5 if task_type.lower() == "io" else cpu_count

    def process_venue(self, venue: dict, years: list, keywords: list):

        results = []

        venue_key = venue['key']
        venue_type = venue['type']
        venue_url = venue['url']
        key_rank_type = f"{venue_key} ({venue['rank']}ç±» {VENUE_TYPES.get(venue_type, 'æœªçŸ¥ç±»å‹')})"

        buffer = StringIO()
        local_logger = logging.getLogger(f"local_{venue['key']}")
        local_logger.setLevel(logging.INFO)

        # ä»…ä¸ºæœ¬åœ° logger æ·»åŠ ä¸´æ—¶ handlerï¼ˆå†…å­˜ä¸­ï¼‰
        buffer_handler = logging.StreamHandler(buffer)
        formatter = logging.Formatter(fmt='%(thread)d - %(asctime)s - %(filename)s[line:%(lineno)d] - %(funcName)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S %p')
        buffer_handler.setFormatter(formatter)
        local_logger.addHandler(buffer_handler)

        local_logger.info(f"ğŸ” å¼€å§‹å¤„ç†ï¼š{key_rank_type}")

        if venue_type == "journal":
            links_by_year = self.get_volume_links_journal(venue_url, years, local_logger)
        elif venue_type == "conference":
            links_by_year = self.get_volume_links_conference(venue_url, years, local_logger)
        else:
            local_logger.warning(f"âš ï¸ æœªçŸ¥ç±»å‹ï¼š{venue_type}ï¼Œè·³è¿‡ {venue_key}")
            return []

        if not links_by_year:
            local_logger.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å·é“¾æ¥ï¼Œè·³è¿‡ {venue_key}")
            return []

        for year, volume_links in links_by_year.items():
            for volume_link in volume_links:
                titles = self.fetch_titles(volume_link, local_logger)
                if not titles:
                    local_logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ–‡ç« æ ‡é¢˜ï¼Œè·³è¿‡ {volume_link}")
                    continue
                for title in titles:
                    if any(keyword.lower() in title.lower() for keyword in keywords):
                        results.append(f"{year}-@-{key_rank_type}-@-{title}")

        local_logger.info(f"ğŸ¯ å®Œæˆï¼š{key_rank_type}ï¼Œå…±æ‰¾åˆ° {len(results)} æ¡åŒ¹é…é¡¹ã€‚")

        # æ‰‹åŠ¨å°† buffer å†…å®¹å†™å…¥ä¸» loggerï¼Œå¹¶æ¸…é™¤ handler
        log_content = buffer.getvalue()
        self.logger.info(f"\n========== æ—¥å¿—åˆ†ç»„ï¼š{key_rank_type} ==========\n{log_content}\n")

        buffer_handler.close()
        local_logger.removeHandler(buffer_handler)
        buffer.close()

        return results

    def run(self, years: list, keywords: list, output_dir: str = "query_result"):

        self.output_dir = output_dir # è¾“å‡ºç›®å½•
        self.output_logs_dir = os.path.join(output_dir, "logs") # æ—¥å¿—ç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"{'-'.join(years)}-{'_'.join([k.replace(' ', '_') for k in keywords])}({timestamp}).txt"
        output_path = os.path.join(self.output_dir, output_filename)

        self.logger = setup_logger("PaperQueryLogger", os.path.join(self.output_logs_dir, output_filename.replace('.txt', '.log')))

        all_results = []

        self.timer.start("æ€»è€—æ—¶")

        with ThreadPoolExecutor(max_workers=self.get_recommended_thread_count("io")) as executor:
            futures = [executor.submit(self.process_venue, venue, years, keywords) for venue in self.venues]
            for future in tqdm(as_completed(futures), total=len(self.venues), desc="æ•´ä½“è¿›åº¦"):
                results = future.result()
                all_results.extend(results)

        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

        with open(output_path, "w", encoding="utf-8") as f:
            f.write("\n".join(all_results))
        self.logger.info(f"âœ… å·²ä¿å­˜ç»“æœåˆ° {output_path}ï¼Œå…±æ‰¾åˆ° {len(all_results)} æ¡åŒ¹é…é¡¹ã€‚")

        elapsed_time = self.timer.stop("æ€»è€—æ—¶")
        self.logger.info(f"â±ï¸ æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")




if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    years = ["2025"]  # æ”¯æŒå¤šä¸ªå¹´ä»½æŸ¥è¯¢
    keywords = [""]  # æ”¯æŒå¤šä¸ªå…³é”®è¯æŸ¥è¯¢

    paper_query = PaperQuery()
    paper_query.run(years, keywords)