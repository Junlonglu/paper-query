import os

import requests
from bs4 import BeautifulSoup
from tqdm import tqdm

from ccf_info_by_section import VENUE_TYPES
from tools.timer import Timer


class PaperQuery:

    def __init__(self, venues:list, output_dir: str = "query_result"):
        """
        åˆå§‹åŒ– PaperQuery ç±»
        """
        self.venues = venues
        self.output_dir = output_dir
        self.timer = Timer()  # åˆå§‹åŒ–è®¡æ—¶å™¨

    def get_soup(self, url):
        """
        è·å–æŒ‡å®š URL çš„ BeautifulSoup å¯¹è±¡
        """
        try:
            res = requests.get(url, timeout=100)
            if res.status_code != 200:
                print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {res.status_code}")
                return None
        except requests.RequestException as e:
            print(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return None
        return BeautifulSoup(res.text, 'lxml')

    def get_volume_links_journal(self, base_url:str, years:list):
        """
        æå–æœŸåˆŠæŒ‡å®šå¹´ä»½çš„æ‰€æœ‰å·é“¾æ¥
        """
        soup = self.get_soup(base_url)
        if not soup:
            print(f"âš ï¸ æ— æ³•è·å–é¡µé¢å†…å®¹ï¼ŒURL: {base_url}")
            return []

        links_by_year = {year: [] for year in years}  # åˆå§‹åŒ–æŒ‰å¹´ä»½åˆ†ç±»çš„å­—å…¸

        info_section = soup.find(id="info-section")
        all_volumes = info_section.find_next_sibling("ul")
        # éå†æ¯ä¸ªå¹´ä»½ï¼Œæ‰¾åˆ°å¯¹åº”çš„å·
        for year in years:
            year_tags = []
            for li in all_volumes.find_all("li"):
                if year in li.get_text(strip=True):
                    year_tags.append(li)

            if not year_tags:
                print(f"âš ï¸ æœªæ‰¾åˆ°å¹´ä»½ {year} çš„å·ä¿¡æ¯")
                continue

            for year_tag in year_tags:
                volume_links = year_tag.find_all("a", href=True)
                for link in volume_links:
                    if link['href'].endswith('.html'):
                        links_by_year[year].append(link['href'])

        return links_by_year

    def get_volume_links_conference(self, base_url:str, years:list):
        """
        æå–ä¼šè®®æŒ‡å®šå¹´ä»½çš„æ‰€æœ‰å·é“¾æ¥
        """
        soup = self.get_soup(base_url)
        if not soup:
            print(f"âš ï¸ æ— æ³•è·å–é¡µé¢å†…å®¹ï¼ŒURL: {base_url}")
            return []

        links_by_year = {year: [] for year in years}  # åˆå§‹åŒ–æŒ‰å¹´ä»½åˆ†ç±»çš„å­—å…¸

        div_main = soup.find(id="main")
        if not div_main:
            print(f"âš ï¸ æ— æ³•æ‰¾åˆ°ä¸»å†…å®¹åŒºåŸŸï¼ŒURL: {base_url}")
            return []

        h2_total = div_main.find_all("h2")
        if not h2_total:
            print(f"âš ï¸ æ— æ³•æ‰¾åˆ°æ ‡é¢˜åŒºåŸŸï¼ŒURL: {base_url}")
            return []

        for year in years:
            for h2 in h2_total:
                id = h2.get('id')
                if id and year in id:
                    parent = h2.find_parent()
                    sibling = parent.find_next_sibling("ul") if parent else None
                    volume_links = sibling.find_all("a", href=True, class_="toc-link") if sibling else []
                    for link in volume_links:
                        if link['href'].endswith('.html'):
                            links_by_year[year].append(link['href'])

        return links_by_year

    def fetch_titles(self, volume_url:str):
        """
        è·å–æ–‡ç« æ ‡é¢˜ï¼ˆè¿”å›æ–‡ç« æ ‡é¢˜åˆ—è¡¨ï¼‰
        """
        soup = self.get_soup(volume_url)
        if not soup:
            print(f"âš ï¸ æ— æ³•è·å–å…·ä½“æ–‡ç« åˆ—è¡¨é¡µå†…å®¹ï¼ŒURL: {volume_url}")
            return []

        articles_containers = soup.find_all("ul", class_="publ-list")
        articles = []
        for container in articles_containers:
            articles.extend(container.find_all("span", class_="title"))

        if not articles:
            print(f"æœªæ‰¾åˆ°æ–‡ç« æ ‡é¢˜ï¼ŒURL: {volume_url}")
            return []

        titles = []
        for article in articles:
            if article.get_text(strip=True).endswith(('.', '?')):
                titles.append(article.get_text(strip=True)[:-1])
            else:
                titles.append(article.get_text(strip=True))
        return titles

    def run(self, years: list, keywords: list):
        """
        ä¸»å‡½æ•°ï¼Œå¤„ç†æŸ¥æ‰¾æ‰€æœ‰æœŸåˆŠ/ä¼šè®®çš„æ–‡ç« 
        """
        output_filename = f"{'_'.join(years)}-{'_'.join([k.replace(' ', '_') for k in keywords])}.txt"
        results = []

        self.timer.start("æ€»è€—æ—¶")  # å¼€å§‹æ€»è€—æ—¶è®¡æ—¶

        for venue in self.venues:
            venue_key = venue['key']
            venue_type = venue["type"]
            venue_url = venue["url"]
            # ç®€ç§°-ç­‰çº§-ç±»å‹
            # ä¾‹å¦‚ï¼šAAAI-A-ä¼šè®®
            keyRankType = f"{venue_key} ({venue['rank']}ç±» {VENUE_TYPES.get(venue_type, 'æœªçŸ¥ç±»å‹')})"

            self.timer.start(f"å¤„ç† {venue_key}")  # å¼€å§‹å¤„ç†å•ä¸ª venue çš„è®¡æ—¶
            print(f"ğŸ” æ­£åœ¨å¤„ç†ï¼š{keyRankType}")

            # æ ¹æ®ç±»å‹åŠ¨æ€è°ƒç”¨å¯¹åº”çš„æ–¹æ³•
            if venue_type == "journal":
                self.timer.start(f"è·å–æœŸåˆŠ {venue_key} ç¬¦åˆç­›é€‰æ¡ä»¶çš„æ‰€æœ‰å·é“¾æ¥")
                links_by_year = self.get_volume_links_journal(venue_url, years)
                self.timer.stop(f"è·å–æœŸåˆŠ {venue_key} ç¬¦åˆç­›é€‰æ¡ä»¶çš„æ‰€æœ‰å·é“¾æ¥")
            elif venue_type == "conference":
                self.timer.start(f"è·å–ä¼šè®® {venue_key} ç¬¦åˆç­›é€‰æ¡ä»¶çš„æ‰€æœ‰å·é“¾æ¥")
                links_by_year = self.get_volume_links_conference(venue_url, years)
                self.timer.stop(f"è·å–ä¼šè®® {venue_key} ç¬¦åˆç­›é€‰æ¡ä»¶çš„æ‰€æœ‰å·é“¾æ¥")
            else:
                print(f"âš ï¸ æœªçŸ¥çš„ venue ç±»å‹ï¼š{venue_type}")
                continue

            if len(links_by_year) == 0:
                print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å·é“¾æ¥ï¼Œè·³è¿‡ {venue_key}")
                continue

            for year, volume_links in links_by_year.items():
                for volume_link in tqdm(volume_links, desc=f"{venue_key} ({year})"):
                    try:
                        titles = self.fetch_titles(volume_link)
                        if len(titles) == 0:
                            continue
                        for title in titles:
                            if any(keyword.lower() in title.lower() for keyword in keywords):
                                results.append(f"{year}-{keyRankType}  {title}")
                    except Exception as e:
                        print(f"[é”™è¯¯] å¤„ç† {volume_link} æ—¶å¤±è´¥: {e}")

            self.timer.stop(f"å¤„ç† {venue_key}\n")  # åœæ­¢å¤„ç†å•ä¸ª venue çš„è®¡æ—¶

        if results:
            if not os.path.exists(self.output_dir):
                os.makedirs(self.output_dir)
            output_filepath = os.path.join(self.output_dir, output_filename)
            with open(output_filepath, "w", encoding="utf-8") as f:
                f.write("\n".join(results))
            print(f"\nâœ… å·²ä¿å­˜ç»“æœåˆ° {output_filepath}ï¼Œå…±æ‰¾åˆ° {len(results)} æ¡åŒ¹é…é¡¹ã€‚")
        else:
            print("\nâš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…ç»“æœã€‚")

        self.timer.stop("æ€»è€—æ—¶")  # åœæ­¢æ€»è€—æ—¶è®¡æ—¶


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    years = ["2025"]  # æ”¯æŒå¤šä¸ªå¹´ä»½æŸ¥è¯¢
    keywords = [""]  # æ”¯æŒå¤šä¸ªå…³é”®è¯æŸ¥è¯¢

    paper_query = PaperQuery()
    paper_query.run(years, keywords)